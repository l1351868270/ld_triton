# ld_triton

triton ops

## suport ops

[attention](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/attention_cn.md)

[flash_attention v1 v2](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/attention_flash_cn.md)

[conv2d](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/conv2d.md)

[convolution](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/convolution.md)

[embedding](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/embedding.md)

[flip](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/flip.md)

[linear](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/linear.md)

[matmul](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/matmul.md)

[max](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/max.md)

[mse](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/mse.md)

[rmsnorm](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/rmsnorm.md)

[rope](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/rope.md)

[sigmoid](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/sigmoid.md)

[softmax](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/softmax.md)

[sparsecon2d](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/conv2d.md)

[sparsecon3d](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/conv2d.md)

[submcon2d](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/conv2d.md)

[submcon3d](https://github.com/l1351868270/ld_triton/blob/main/docs/ops/conv2d.md)

## distributed
[data_parallel](https://github.com/l1351868270/ld_triton/blob/main/docs/distributed/data_parallel.md)

[pipeline_parallel](https://github.com/l1351868270/ld_triton/blob/main/docs/distributed/pipeline_parallel.md)

[PyTorch Symmetric Memory](https://github.com/l1351868270/ld_triton/blob/main/docs/distributed/symmetric_memory.md)

## models
[qwen](https://github.com/l1351868270/ld_triton/blob/main/docs/model/qwen.md)
